{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:51:10.157233Z","iopub.status.busy":"2023-04-01T06:51:10.156003Z","iopub.status.idle":"2023-04-01T06:51:32.640493Z","shell.execute_reply":"2023-04-01T06:51:32.639041Z","shell.execute_reply.started":"2023-04-01T06:51:10.157132Z"},"trusted":true},"outputs":[],"source":["# !pip install transformers -Uqq"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:51:32.647599Z","iopub.status.busy":"2023-04-01T06:51:32.646740Z","iopub.status.idle":"2023-04-01T06:51:34.728558Z","shell.execute_reply":"2023-04-01T06:51:34.727385Z","shell.execute_reply.started":"2023-04-01T06:51:32.647548Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:51:34.736973Z","iopub.status.busy":"2023-04-01T06:51:34.734057Z","iopub.status.idle":"2023-04-01T06:51:43.326804Z","shell.execute_reply":"2023-04-01T06:51:43.325657Z","shell.execute_reply.started":"2023-04-01T06:51:34.736912Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import json\n","import argparse\n","\n","# Importing some tools for flattening 2d arrays to 1d\n","from functools import reduce\n","from operator import add\n","# Importing hugging face library for getting the transformers and tokenizers\n","from transformers import AutoTokenizer, BartForConditionalGeneration,PegasusForConditionalGeneration,AdamW,get_linear_schedule_with_warmup\n","\n","# Importing some pytorch classes and functions\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\n","from torch import cuda "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:51:43.330328Z","iopub.status.busy":"2023-04-01T06:51:43.329929Z","iopub.status.idle":"2023-04-01T06:51:55.143669Z","shell.execute_reply":"2023-04-01T06:51:55.142207Z","shell.execute_reply.started":"2023-04-01T06:51:43.330289Z"},"trusted":true},"outputs":[],"source":["%%capture\n","# !pip install pytorch-lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:51:55.147632Z","iopub.status.busy":"2023-04-01T06:51:55.146730Z","iopub.status.idle":"2023-04-01T06:51:56.845189Z","shell.execute_reply":"2023-04-01T06:51:56.844129Z","shell.execute_reply.started":"2023-04-01T06:51:55.147579Z"},"trusted":true},"outputs":[],"source":["import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping\n","from pytorch_lightning.plugins import DeepSpeedPlugin"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:51:56.847534Z","iopub.status.busy":"2023-04-01T06:51:56.846620Z","iopub.status.idle":"2023-04-01T06:52:15.280793Z","shell.execute_reply":"2023-04-01T06:52:15.279285Z","shell.execute_reply.started":"2023-04-01T06:51:56.847499Z"},"trusted":true},"outputs":[],"source":["%%capture\n","# !pip install deepspeed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:15.285335Z","iopub.status.busy":"2023-04-01T06:52:15.284301Z","iopub.status.idle":"2023-04-01T06:52:21.840887Z","shell.execute_reply":"2023-04-01T06:52:21.839500Z","shell.execute_reply.started":"2023-04-01T06:52:15.285292Z"},"trusted":true},"outputs":[],"source":["# from deepspeed.ops.adam import FusedAdam"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:21.848460Z","iopub.status.busy":"2023-04-01T06:52:21.845306Z","iopub.status.idle":"2023-04-01T06:52:44.178091Z","shell.execute_reply":"2023-04-01T06:52:44.176716Z","shell.execute_reply.started":"2023-04-01T06:52:21.848413Z"},"trusted":true},"outputs":[],"source":["!pip install rouge\n","!pip install bert_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.180542Z","iopub.status.busy":"2023-04-01T06:52:44.180120Z","iopub.status.idle":"2023-04-01T06:52:44.264118Z","shell.execute_reply":"2023-04-01T06:52:44.263226Z","shell.execute_reply.started":"2023-04-01T06:52:44.180493Z"},"trusted":true},"outputs":[],"source":["# Importing libraries for evaluation\n","from rouge import Rouge\n","from bert_score import score\n","\n","\n","# Importing library for progress bar GUI\n","#from fastai.text.core import progress_bar\n","from tqdm import tqdm\n","\n","# Importing library for parallization\n","from joblib import Parallel,delayed,Memory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.268338Z","iopub.status.busy":"2023-04-01T06:52:44.268035Z","iopub.status.idle":"2023-04-01T06:52:44.354397Z","shell.execute_reply":"2023-04-01T06:52:44.353333Z","shell.execute_reply.started":"2023-04-01T06:52:44.268311Z"},"trusted":true},"outputs":[],"source":["device = 'cuda' if cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.356891Z","iopub.status.busy":"2023-04-01T06:52:44.356066Z","iopub.status.idle":"2023-04-01T06:52:44.884066Z","shell.execute_reply":"2023-04-01T06:52:44.882984Z","shell.execute_reply.started":"2023-04-01T06:52:44.356849Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/mtpdataset/dataset1920.csv')\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.886359Z","iopub.status.busy":"2023-04-01T06:52:44.885405Z","iopub.status.idle":"2023-04-01T06:52:44.891171Z","shell.execute_reply":"2023-04-01T06:52:44.890049Z","shell.execute_reply.started":"2023-04-01T06:52:44.886317Z"},"trusted":true},"outputs":[],"source":["# import string\n","# string.punctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.893654Z","iopub.status.busy":"2023-04-01T06:52:44.892679Z","iopub.status.idle":"2023-04-01T06:52:44.901739Z","shell.execute_reply":"2023-04-01T06:52:44.900563Z","shell.execute_reply.started":"2023-04-01T06:52:44.893612Z"},"trusted":true},"outputs":[],"source":["# def remove_punctuation(text):\n","#     punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n","#     return punctuationfree\n","# #storing the puntuation free text\n","# data['Cleaned_OriginalTxt']= data['Original_Text'].apply(lambda x:remove_punctuation(x))\n","# data['Cleaned_Summary']= data['Summary'].apply(lambda x:remove_punctuation(x))\n","# data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.904414Z","iopub.status.busy":"2023-04-01T06:52:44.903213Z","iopub.status.idle":"2023-04-01T06:52:44.911281Z","shell.execute_reply":"2023-04-01T06:52:44.909993Z","shell.execute_reply.started":"2023-04-01T06:52:44.904371Z"},"trusted":true},"outputs":[],"source":["# data['Cleaned_OriginalTxt']= data['Cleaned_OriginalTxt'].apply(lambda x: x.lower())\n","# data['Cleaned_Summary']= data['Cleaned_Summary'].apply(lambda x: x.lower())\n","# data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.914841Z","iopub.status.busy":"2023-04-01T06:52:44.912320Z","iopub.status.idle":"2023-04-01T06:52:44.924192Z","shell.execute_reply":"2023-04-01T06:52:44.923114Z","shell.execute_reply.started":"2023-04-01T06:52:44.914770Z"},"trusted":true},"outputs":[],"source":["# import re\n","# def remove_urls(text):\n","#     url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","#     return url_pattern.sub(r'', text)\n","# data['Cleaned_OriginalTxt']= data['Cleaned_OriginalTxt'].apply(lambda x:remove_urls(x))\n","# data['Cleaned_Summary']= data['Cleaned_Summary'].apply(lambda x:remove_urls(x))\n","# data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.927887Z","iopub.status.busy":"2023-04-01T06:52:44.927544Z","iopub.status.idle":"2023-04-01T06:52:44.937950Z","shell.execute_reply":"2023-04-01T06:52:44.936874Z","shell.execute_reply.started":"2023-04-01T06:52:44.927838Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","# Split the dataset into training and testing sets\n","train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.940178Z","iopub.status.busy":"2023-04-01T06:52:44.939629Z","iopub.status.idle":"2023-04-01T06:52:44.959439Z","shell.execute_reply":"2023-04-01T06:52:44.958277Z","shell.execute_reply.started":"2023-04-01T06:52:44.940138Z"},"trusted":true},"outputs":[],"source":["train_data.applymap(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:52:44.962022Z","iopub.status.busy":"2023-04-01T06:52:44.961615Z","iopub.status.idle":"2023-04-01T06:52:44.969860Z","shell.execute_reply":"2023-04-01T06:52:44.968518Z","shell.execute_reply.started":"2023-04-01T06:52:44.961981Z"},"trusted":true},"outputs":[],"source":["train_summ = list(train_data['Summary'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:53:04.837835Z","iopub.status.busy":"2023-04-01T06:53:04.837428Z","iopub.status.idle":"2023-04-01T06:53:04.845189Z","shell.execute_reply":"2023-04-01T06:53:04.843693Z","shell.execute_reply.started":"2023-04-01T06:53:04.837799Z"},"trusted":true},"outputs":[],"source":["# Getting the train, and test data splitted using splitting_text function declared above\n","train_text,train_summ = list(train_data.Original_Text) , train_summ\n","test_text = list(test_data.Original_Text)\n","val_text,val_summ = train_text[:100], train_summ[:100]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T06:53:06.534123Z","iopub.status.busy":"2023-04-01T06:53:06.533710Z","iopub.status.idle":"2023-04-01T06:53:06.540922Z","shell.execute_reply":"2023-04-01T06:53:06.539625Z","shell.execute_reply.started":"2023-04-01T06:53:06.534086Z"},"trusted":true},"outputs":[],"source":["print(len(train_text))\n","print(len(train_summ))\n","print(len(test_text))\n","print(len(val_text))\n","print(len(val_summ))"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenization\n","Tokenization Class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:01:27.994885Z","iopub.status.busy":"2023-04-01T07:01:27.993907Z","iopub.status.idle":"2023-04-01T07:01:28.014225Z","shell.execute_reply":"2023-04-01T07:01:28.012556Z","shell.execute_reply.started":"2023-04-01T07:01:27.994844Z"},"trusted":true},"outputs":[],"source":["from joblib import Parallel, delayed\n","from functools import reduce\n","from operator import add\n","from tqdm import tqdm\n","import torch\n","\n","\n","class TransformersBaseTokenizer:\n","\n","    \"\"\"Class for encoding and decoding given texts for transformers\"\"\"\n","\n","    def __init__(\n","        self,\n","        pretrained_tokenizer,\n","        model_type='bart',\n","        **kwargs\n","        ):\n","        self._pretrained_tokenizer = pretrained_tokenizer\n","        self.max_seq_len = pretrained_tokenizer.model_max_length\n","        self.model_type = model_type\n","        self.pad_token_id = pretrained_tokenizer.pad_token_id\n","\n","    def __call__(self, *args, **kwargs):\n","        return self\n","\n","    def tokenizer(self, t):\n","        \"\"\"Limits the maximum sequence length and add the special tokens\"\"\"\n","\n","        if self.model_type == 'bart':\n","            \n","            CLS = self._pretrained_tokenizer.cls_token\n","            SEP = self._pretrained_tokenizer.sep_token\n","            \n","            tokens = \\\n","                self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len\n","                - 2]\n","            tokens = [CLS] + tokens + [SEP]\n","\n","        elif self.model_type == 'pegasus':\n","            eos = self._pretrained_tokenizer.eos_token\n","            tokens = \\\n","                self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len\n","                - 2]\n","            tokens = tokens + [eos]\n","                    \n","\n","        return tokens\n","\n","    def _numercalise(self, t):\n","        \"\"\"Convert text to their corresponding ids\"\"\"\n","        \n","        tokenized_text = self._pretrained_tokenizer(\n","                t,\n","                max_length=self.max_seq_len,\n","                return_tensors='pt',\n","                padding='max_length',\n","                truncation=True,\n","                add_special_tokens=True,\n","                is_split_into_words=False,\n","                )\n","        return tokenized_text\n","\n","    def _textify(self, input_ids):\n","        \"\"\"Convert ids to their corresponding text\"\"\"\n","\n","        text = self._pretrained_tokenizer.batch_decode(input_ids,\n","                skip_special_tokens=True,\n","                clean_up_tokenization_spaces=False)\n","        return text\n","\n","    def _chunks(self, lst, n):\n","        \"\"\"Splitting the text into batches\"\"\"\n","\n","        for i in range(0, len(lst), n):\n","            yield lst[i:i + n]\n","\n","    def numercalise(self, t, batch_size=4):\n","        \"\"\"Convert text to their corresponding ids and get the attention mask to differentiate between pad and input texts\"\"\"\n","\n","        if isinstance(t, str):\n","            t = [t]  # convert str to list of str\n","\n","        n_cores = min(batch_size, os.cpu_count())\n","        results = Parallel(n_jobs=n_cores)(delayed(self._numercalise)(batch)\n","                for batch in tqdm(list(self._chunks(t,\n","                batch_size))))\n","        input_ids = []\n","        attention_masks = []\n","        for i in results:\n","            input_ids.append(i['input_ids'])\n","            attention_masks.append(i['attention_mask'])\n","\n","        return {'input_ids': torch.cat(input_ids),\n","                'attention_mask': torch.cat(attention_masks)}\n","\n","    def textify(self, tensors, batch_size):\n","        \"\"\"Convert ids to their corresponding text\"\"\"\n","\n","        if len(tensors.shape) == 1:\n","            tensors = [tensors]  # convert 1d tensor to 2d\n","\n","        n_cores = min(batch_size, os.cpu_count())\n","        results = Parallel(n_jobs=-1, backend='threading'\n","                           )(delayed(self._textify)(summary_ids)\n","                             for summary_ids in\n","                             tqdm(list(self._chunks(tensors,\n","                             batch_size))))\n","\n","        return reduce(add, results)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:01:32.470873Z","iopub.status.busy":"2023-04-01T07:01:32.470494Z","iopub.status.idle":"2023-04-01T07:01:32.477891Z","shell.execute_reply":"2023-04-01T07:01:32.476623Z","shell.execute_reply.started":"2023-04-01T07:01:32.470817Z"},"trusted":true},"outputs":[],"source":["# # ORIGINAL TransformersBaseTokenizer FUNCTION\n","# class TransformersBaseTokenizer:\n","\n","#     \"\"\"Class for encoding and decoding given texts for transformers\"\"\"\n","\n","#     def __init__(\n","#         self,\n","#         pretrained_tokenizer,\n","#         model_type='bart',\n","#         **kwargs\n","#         ):\n","#         self._pretrained_tokenizer = pretrained_tokenizer\n","#         self.max_seq_len = pretrained_tokenizer.model_max_length\n","#         self.model_type = model_type\n","#         self.pad_token_id = pretrained_tokenizer.pad_token_id\n","\n","#     def __call__(self, *args, **kwargs):\n","#         return self\n","\n","#     def tokenizer(self, t):\n","#         \"\"\"Limits the maximum sequence length and add the special tokens\"\"\"\n","\n","#         if self.model_type == 'bart':\n","            \n","#             CLS = self._pretrained_tokenizer.cls_token\n","#             SEP = self._pretrained_tokenizer.sep_token\n","            \n","#             tokens = \\\n","#                 self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len\n","#                 - 2]\n","#             tokens = [CLS] + tokens + [SEP]\n","\n","#         elif self.model_type == 'pegasus':\n","#             eos = self._pretrained_tokenizer.eos_token\n","#             tokens = \\\n","#                 self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len\n","#                 - 2]\n","#             tokens = tokens + [eos]\n","                    \n","\n","#         return tokens\n","\n","#     def _numercalise(self, t):\n","#         \"\"\"Convert text to there coressponding ids\"\"\"\n","        \n","#         tokenized_text = self._pretrained_tokenizer(\n","#                 t,\n","#                 max_length=self.max_seq_len,\n","#                 return_tensors='pt',\n","#                 padding='max_length',\n","#                 truncation=True,\n","#                 add_special_tokens=True,\n","#                 is_split_into_words=False,\n","#                 )\n","#         return tokenized_text\n","\n","#     def _textify(self, input_ids):\n","#         \"\"\"Convert ids to thier coressponding text\"\"\"\n","\n","#         text = self._pretrained_tokenizer.batch_decode(input_ids,\n","#                 skip_special_tokens=True,\n","#                 clean_up_tokenization_spaces=False)\n","#         return text\n","\n","#     def _chunks(self, lst, n):\n","#         \"\"\"splitting the text into batches\"\"\"\n","\n","#         for i in range(0, len(lst), n):\n","#             yield lst[i:i + n]\n","\n","#     def numercalise(self, t, batch_size=4):\n","#         \"\"\"Convert text to thier coressponding ids and get the attention mask to differentiate between pad and input texts\"\"\"\n","\n","#         if isinstance(t, str):\n","#             t = [t]  # convert str to list of str\n","\n","#         results = Parallel(n_jobs=-1)(delayed(self._numercalise)(batch)\n","#                 for batch in tqdm(list(self._chunks(t,\n","#                 batch_size))))\n","#         input_ids = []\n","#         attention_masks = []\n","#         for i in results:\n","#             input_ids.append(i['input_ids'])\n","#             attention_masks.append(i['attention_mask'])\n","\n","#         return {'input_ids': torch.cat(input_ids),\n","#                 'attention_mask': torch.cat(attention_masks)}\n","\n","#     def textify(self, tensors, batch_size):\n","#         \"\"\"Convert ids to thier coressponding text\"\"\"\n","\n","#         if len(tensors.shape) == 1:\n","#             tensors = [tensors]  # convert 1d tensor to 2d\n","\n","#         results = Parallel(n_jobs=-1, backend='threading'\n","#                            )(delayed(self._textify)(summary_ids)\n","#                              for summary_ids in\n","#                              tqdm(list(self._chunks(tensors,\n","#                              batch_size))))\n","\n","#         return reduce(add, results)"]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizing data for BART Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:01:34.467094Z","iopub.status.busy":"2023-04-01T07:01:34.466042Z","iopub.status.idle":"2023-04-01T07:01:34.680909Z","shell.execute_reply":"2023-04-01T07:01:34.679870Z","shell.execute_reply.started":"2023-04-01T07:01:34.467042Z"},"trusted":true},"outputs":[],"source":["# Download the bart tokenizer from hugging face api\n","bart_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-xsum')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:01:36.102285Z","iopub.status.busy":"2023-04-01T07:01:36.100506Z","iopub.status.idle":"2023-04-01T07:01:36.118918Z","shell.execute_reply":"2023-04-01T07:01:36.117656Z","shell.execute_reply.started":"2023-04-01T07:01:36.102234Z"},"trusted":true},"outputs":[],"source":["# Passing the bart tokenizer to our TransformersBaseTokenizer wrapper\n","tokenizer = TransformersBaseTokenizer(bart_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:01:54.668389Z","iopub.status.busy":"2023-04-01T07:01:54.667973Z","iopub.status.idle":"2023-04-01T07:01:54.673551Z","shell.execute_reply":"2023-04-01T07:01:54.672287Z","shell.execute_reply.started":"2023-04-01T07:01:54.668353Z"},"trusted":true},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:01:55.808568Z","iopub.status.busy":"2023-04-01T07:01:55.807506Z","iopub.status.idle":"2023-04-01T07:02:38.532608Z","shell.execute_reply":"2023-04-01T07:02:38.531401Z","shell.execute_reply.started":"2023-04-01T07:01:55.808530Z"},"trusted":true},"outputs":[],"source":["# Converting the text into there coressponding input_ids and attention_mask to be interperted by the model\n","\n","train_inputs = tokenizer.numercalise(train_text,16)\n","\n","val_inputs = tokenizer.numercalise(val_text,16)\n","\n","test_inputs = tokenizer.numercalise(test_text,16)\n","\n","#train_outputs = tokenizer.numercalise(reduce(add,train_summ),16)\n","\n","#val_outputs = tokenizer.numercalise(reduce(add,val_summ),16)\n","\n","val_outputs = tokenizer.numercalise(val_summ,16)\n","train_outputs = tokenizer.numercalise(train_summ,16)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:03:43.643711Z","iopub.status.busy":"2023-04-01T07:03:43.643148Z","iopub.status.idle":"2023-04-01T07:03:43.653336Z","shell.execute_reply":"2023-04-01T07:03:43.651920Z","shell.execute_reply.started":"2023-04-01T07:03:43.643658Z"},"trusted":true},"outputs":[],"source":["print(train_inputs['input_ids'].shape)\n","print(train_inputs['attention_mask'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:03:46.363877Z","iopub.status.busy":"2023-04-01T07:03:46.363501Z","iopub.status.idle":"2023-04-01T07:03:46.369568Z","shell.execute_reply":"2023-04-01T07:03:46.368230Z","shell.execute_reply.started":"2023-04-01T07:03:46.363843Z"},"trusted":true},"outputs":[],"source":["# Getting the labels from train and val\n","labels = train_outputs['input_ids']\n","val_labels = val_outputs['input_ids']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:03:46.711412Z","iopub.status.busy":"2023-04-01T07:03:46.710231Z","iopub.status.idle":"2023-04-01T07:03:46.717031Z","shell.execute_reply":"2023-04-01T07:03:46.715764Z","shell.execute_reply.started":"2023-04-01T07:03:46.711320Z"},"trusted":true},"outputs":[],"source":["train_inputs['labels'] = labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:03:47.016906Z","iopub.status.busy":"2023-04-01T07:03:47.016479Z","iopub.status.idle":"2023-04-01T07:03:47.024487Z","shell.execute_reply":"2023-04-01T07:03:47.023559Z","shell.execute_reply.started":"2023-04-01T07:03:47.016874Z"},"trusted":true},"outputs":[],"source":["val_inputs['labels'] = val_labels"]},{"cell_type":"markdown","metadata":{},"source":["## Model Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:04:55.914447Z","iopub.status.busy":"2023-04-01T07:04:55.913900Z","iopub.status.idle":"2023-04-01T07:04:55.924815Z","shell.execute_reply":"2023-04-01T07:04:55.923668Z","shell.execute_reply.started":"2023-04-01T07:04:55.914393Z"},"trusted":true},"outputs":[],"source":["hparams = argparse.Namespace()\n","\n","hparams.freeze_encoder = True\n","hparams.freeze_embeds = True\n","hparams.eval_beams = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:13:46.361622Z","iopub.status.busy":"2023-04-01T07:13:46.361114Z","iopub.status.idle":"2023-04-01T07:13:46.371441Z","shell.execute_reply":"2023-04-01T07:13:46.370202Z","shell.execute_reply.started":"2023-04-01T07:13:46.361578Z"},"trusted":true},"outputs":[],"source":["def shift_tokens_right(input_ids, pad_token_id):\n","    \"\"\" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n","      This is taken directly from modeling_bart.py\n","  \"\"\"\n","\n","    prev_output_tokens = input_ids.clone()\n","    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1)\n","                    - 1).unsqueeze(-1)\n","    prev_output_tokens[:, 0] = input_ids.gather(1,\n","            index_of_eos).squeeze()\n","    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n","    \n","    return prev_output_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:06:12.650836Z","iopub.status.busy":"2023-04-01T07:06:12.650426Z","iopub.status.idle":"2023-04-01T07:06:12.656771Z","shell.execute_reply":"2023-04-01T07:06:12.655685Z","shell.execute_reply.started":"2023-04-01T07:06:12.650804Z"},"trusted":true},"outputs":[],"source":["def freeze_params(model):\n","    ''' Making the input part of the model as non trainable parameters'''\n","    # for name, layer in list(model.named_parameters()):#[:-3]:\n","    #     layer.requires_grad = False\n","    for layer in list(model.parameters())[:-1]:\n","      layer.requires_grad=False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:06:14.503048Z","iopub.status.busy":"2023-04-01T07:06:14.502618Z","iopub.status.idle":"2023-04-01T07:06:14.522348Z","shell.execute_reply":"2023-04-01T07:06:14.521284Z","shell.execute_reply.started":"2023-04-01T07:06:14.503009Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/q-learningtry/final_tokens.json', 'r') as f:\n","    data1 = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:06:17.341110Z","iopub.status.busy":"2023-04-01T07:06:17.340343Z","iopub.status.idle":"2023-04-01T07:06:17.347970Z","shell.execute_reply":"2023-04-01T07:06:17.346851Z","shell.execute_reply.started":"2023-04-01T07:06:17.341070Z"},"trusted":true},"outputs":[],"source":["forced_tokens11 = data1\n","len(forced_tokens11)"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing list of tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:06:24.512874Z","iopub.status.busy":"2023-04-01T07:06:24.511819Z","iopub.status.idle":"2023-04-01T07:06:25.058100Z","shell.execute_reply":"2023-04-01T07:06:25.057017Z","shell.execute_reply.started":"2023-04-01T07:06:24.512829Z"},"trusted":true},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","\n","# convert to lowercase\n","words = [word.lower() for word in forced_tokens11]\n","\n","# remove punctuation\n","import string\n","words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) for word in words]\n","\n","# remove stop words\n","stop_words = set(stopwords.words('english'))\n","forced_tokens1 = [word for word in words if not word in stop_words]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:08:57.395042Z","iopub.status.busy":"2023-04-01T07:08:57.394031Z","iopub.status.idle":"2023-04-01T07:08:57.410739Z","shell.execute_reply":"2023-04-01T07:08:57.409355Z","shell.execute_reply.started":"2023-04-01T07:08:57.394994Z"},"trusted":true},"outputs":[],"source":["import re\n","def preprocess_tokens(tokens):\n","    \"\"\"Preprocesses a list of tokens to keep only words and remove non-word characters\"\"\"\n","    return [re.sub(r'\\W+', '', token) for token in tokens if re.match(r'^\\w+$', token)]\n","forced_tokens1 = preprocess_tokens(forced_tokens1)\n","len(forced_tokens1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:09:07.146670Z","iopub.status.busy":"2023-04-01T07:09:07.146275Z","iopub.status.idle":"2023-04-01T07:09:07.466134Z","shell.execute_reply":"2023-04-01T07:09:07.465015Z","shell.execute_reply.started":"2023-04-01T07:09:07.146635Z"},"trusted":true},"outputs":[],"source":["class Model(pl.LightningModule):\n","\n","    def __init__(\n","        self,\n","        lr,\n","        tokenizer,\n","        model,\n","        params,\n","        n_warmup_steps=None,\n","        n_training_steps =None\n","        ):\n","\n","        super(Model, self).__init__()\n","\n","        self.tokenizer = tokenizer\n","        self.model = model\n","        self.lr = lr\n","        self.params = params\n","        self.n_warmup_steps = n_warmup_steps\n","        self.n_training_steps = n_training_steps\n","        if self.params.freeze_encoder:\n","            freeze_params(self.model.get_encoder())\n","\n","        if self.params.freeze_embeds:\n","            self.freeze_embeds()\n","\n","    def freeze_embeds(self):\n","        ''' freeze the positional embedding parameters of the model '''\n","\n","        freeze_params(self.model.model.shared)\n","        for d in [self.model.model.encoder, self.model.model.decoder]:\n","            freeze_params(d.embed_positions)\n","            freeze_params(d.embed_tokens)\n","\n","    def forward(self, input_ids, **kwargs):\n","        return self.model(input_ids, **kwargs)\n","\n","    def configure_optimizers(self):\n","        optimizer = FusedAdam(self.parameters(), lr=self.lr)\n","        scheduler = get_linear_schedule_with_warmup(\n","                                                      optimizer,\n","                                                      num_warmup_steps=self.n_warmup_steps,\n","                                                      num_training_steps=self.n_training_steps\n","                                                    )\n","        \n","        return dict(optimizer=optimizer,lr_scheduler=dict(scheduler=scheduler,interval='step'))\n","\n","    \n","    def training_step(self, batch, batch_idx):\n","        # Load the data into variables\n","        src_ids, src_mask = batch[0], batch[1]\n","        tgt_ids = batch[2]\n","\n","        # Shift the decoder tokens right (but NOT the tgt_ids)\n","        decoder_input_ids = shift_tokens_right(tgt_ids,\n","                    self.tokenizer.pad_token_id)\n","\n","        # Run the model and get the logits\n","        outputs = self(src_ids, attention_mask=src_mask,\n","                       decoder_input_ids=decoder_input_ids,\n","                       use_cache=False)\n","        labels_logits = outputs[0]\n","\n","        # Define the forced tokens and corresponding indices\n","        forced_tokens = forced_tokens1\n","        forced_token_idx = {self.tokenizer.encode(token)[0]: i for i, token in enumerate(forced_tokens)}\n","\n","        # Calculate the loss on the un-shifted tokens\n","        ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n","        ce_loss = ce_loss_fct(labels_logits.view(-1, labels_logits.shape[-1]), tgt_ids.view(-1))\n","\n","        # Define the forced token penalty function\n","        def forced_token_penalty(output):\n","            penalty = 0.0\n","            for token, idx in forced_token_idx.items():\n","                if idx not in output:\n","                    # Penalize the model if the forced token is absent\n","                    penalty += 1.0\n","            return penalty\n","\n","        # Calculate the forced token penalty\n","        penalty = forced_token_penalty(tgt_ids)\n","\n","        # Combine the loss and penalty terms with a weighting factor\n","        lambda_weight = 0.5\n","        loss = lambda_weight * ce_loss + (1 - lambda_weight) * penalty\n","\n","        # Log the loss values\n","        self.log('train_loss/epoch', loss, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n","        self.log('train_loss/step', loss, prog_bar=True, logger=True, on_epoch=False, on_step=True)\n","\n","        return {\"loss\": loss}\n","\n","    \n","    def validation_step(self, batch, batch_idx):\n","        # Load the data into variables\n","        src_ids, src_mask = batch[0], batch[1]\n","        tgt_ids = batch[2]\n","\n","        # Shift the decoder tokens right (but NOT the tgt_ids)\n","        decoder_input_ids = shift_tokens_right(tgt_ids, self.tokenizer.pad_token_id)\n","\n","        # Run the model and get the logits\n","        outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n","        labels_logits = outputs[0]\n","\n","        # Define the forced tokens and corresponding indices\n","        forced_tokens = forced_tokens1\n","        forced_token_idx = {self.tokenizer.encode(token)[0]: i for i, token in enumerate(forced_tokens)}\n","\n","        # Calculate the loss on the un-shifted tokens\n","        ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n","        ce_loss = ce_loss_fct(labels_logits.view(-1, labels_logits.shape[-1]), tgt_ids.view(-1))\n","\n","        # Define the forced token penalty function\n","        def forced_token_penalty(output):\n","            penalty = 0.0\n","            for token, idx in forced_token_idx.items():\n","                if idx not in output:\n","                    # Penalize the model if the forced token is absent\n","                    penalty += 1.0\n","            return penalty\n","\n","        # Calculate the forced token penalty\n","        penalty = forced_token_penalty(tgt_ids)\n","\n","        # Combine the loss and penalty terms with a weighting factor\n","        lambda_weight = 0.5\n","        loss = lambda_weight * ce_loss + (1 - lambda_weight) * penalty\n","\n","        # Log the loss values\n","        self.log('val_loss/epoch', loss, prog_bar=True, logger=True, on_epoch=True, on_step=False)\n","        self.log('val_loss/step', loss, prog_bar=True, logger=True, on_epoch=False, on_step=True)\n","\n","        return {'loss': loss}\n","\n","    \n","    def _chunks(self, lst, n):\n","        \"\"\"splitting the text into batches\"\"\"\n","\n","        for i in range(0, len(lst['input_ids']), n):\n","            yield lst['input_ids'][i:i + n],lst['attention_mask'][i:i + n]\n","\n","    def _generate_text(\n","        self,\n","        text,\n","        mask,\n","        eval_beams,\n","        early_stopping=True,\n","        max_len=150,\n","        penalty_length = 0.2\n","    ):\n","        \n","        generated_ids = self.model.generate(\n","            text.to(device),\n","            attention_mask=mask.to(device),\n","            use_cache=True,\n","            decoder_start_token_id=self.tokenizer.pad_token_id,\n","            num_beams=eval_beams,\n","            max_length=max_len,\n","            early_stopping=early_stopping,\n","            length_penalty = penalty_length,\n","            no_repeat_ngram_size=3\n","            \n","            )\n","        \n","        return [self.tokenizer.decode(w, skip_special_tokens=True,\n","                clean_up_tokenization_spaces=True) for w in\n","                generated_ids]\n","\n","    def generate_text(\n","        self,\n","        text,\n","        eval_beams,\n","        early_stopping=True,\n","        max_len=250,\n","        batch_size=2,\n","        length_penalty = 0.2\n","        ):\n","        ''' Function to generate text '''\n","        summaries = []\n","        \n","        for ids,mask in tqdm(list(self._chunks(text,batch_size))):\n","            txt = self._generate_text(ids,mask=mask,eval_beams = eval_beams,early_stopping = early_stopping,max_len = max_len,penalty_length = length_penalty)\n","            print(\"Summary gen\\n\")\n","            print(txt)\n","            print(\"\\n\")\n","            summaries.extend(txt)\n","    \n","        return summaries\n","            \n","\n","\n","    def save(self, model_name):\n","        model_extension =  model_name + '.h5'\n","        torch.save(self.model,model_extension)\n","        print ('Model is saved')\n","        return './'+  model_extension"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:09:12.860403Z","iopub.status.busy":"2023-04-01T07:09:12.859590Z","iopub.status.idle":"2023-04-01T07:09:12.873431Z","shell.execute_reply":"2023-04-01T07:09:12.872018Z","shell.execute_reply.started":"2023-04-01T07:09:12.860359Z"},"trusted":true},"outputs":[],"source":["# Create a dataloading module as per the PyTorch Lightning Docs\n","\n","class SummaryDataModule(pl.LightningDataModule):\n","\n","    def __init__(\n","        self,\n","        train,\n","        val=None,\n","        test=None,\n","        batch_size=2,\n","        ):\n","        super().__init__()\n","        self.train = train\n","        self.val = val\n","        self.test = test\n","        self.batch_size = batch_size\n","\n","  # Load the training, validation and test sets in Pytorch Dataset objects\n","\n","    def train_dataloader(self):\n","        dataset = TensorDataset(self.train['input_ids'],\n","                                self.train['attention_mask'],\n","                                self.train['labels'])\n","        train_data = DataLoader(dataset,\n","                                sampler=RandomSampler(dataset),\n","                                batch_size=self.batch_size)\n","        return train_data\n","\n","    def val_dataloader(self):\n","        dataset = TensorDataset(self.val['input_ids'],\n","                                self.val['attention_mask'],\n","                                self.val['labels'])\n","        val_data = DataLoader(dataset, batch_size=self.batch_size)\n","        return val_data\n","\n","    def test_dataloader(self):\n","        dataset = TensorDataset(self.test['input_ids'],\n","                                self.test['attention_mask'],\n","                                self.test['labels'])\n","        test_data = DataLoader(dataset, batch_size=self.batch_size)\n","        return test_data"]},{"cell_type":"markdown","metadata":{},"source":["## BART Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:09:53.918152Z","iopub.status.busy":"2023-04-01T07:09:53.916848Z","iopub.status.idle":"2023-04-01T07:09:53.923621Z","shell.execute_reply":"2023-04-01T07:09:53.922297Z","shell.execute_reply.started":"2023-04-01T07:09:53.918091Z"},"trusted":true},"outputs":[],"source":["# Load the data into the model for training\n","summary_data = SummaryDataModule(train = train_inputs, val=val_inputs,\n","                                 batch_size=  2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:09:57.516704Z","iopub.status.busy":"2023-04-01T07:09:57.516183Z","iopub.status.idle":"2023-04-01T07:10:27.864039Z","shell.execute_reply":"2023-04-01T07:10:27.862330Z","shell.execute_reply.started":"2023-04-01T07:09:57.516658Z"},"trusted":true},"outputs":[],"source":["bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-xsum\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:10:27.872034Z","iopub.status.busy":"2023-04-01T07:10:27.871610Z","iopub.status.idle":"2023-04-01T07:10:27.882224Z","shell.execute_reply":"2023-04-01T07:10:27.880984Z","shell.execute_reply.started":"2023-04-01T07:10:27.871976Z"},"trusted":true},"outputs":[],"source":["steps_per_epoch=len(train_text) // 1\n","total_training_steps = steps_per_epoch * 5\n","warmup_steps = total_training_steps // 5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:11:28.132709Z","iopub.status.busy":"2023-04-01T07:11:28.131963Z","iopub.status.idle":"2023-04-01T07:11:28.191069Z","shell.execute_reply":"2023-04-01T07:11:28.190086Z","shell.execute_reply.started":"2023-04-01T07:11:28.132672Z"},"trusted":true},"outputs":[],"source":["model = Model(lr = 2e-5, tokenizer = bart_tokenizer, model = bart_model, params = hparams,  n_warmup_steps=warmup_steps,\n","  n_training_steps=total_training_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:11:30.543113Z","iopub.status.busy":"2023-04-01T07:11:30.542662Z","iopub.status.idle":"2023-04-01T07:11:30.548183Z","shell.execute_reply":"2023-04-01T07:11:30.547025Z","shell.execute_reply.started":"2023-04-01T07:11:30.543073Z"},"trusted":true},"outputs":[],"source":["# early_stopping_callback = EarlyStopping(monitor='val_loss/epoch', patience=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:11:30.756241Z","iopub.status.busy":"2023-04-01T07:11:30.755032Z","iopub.status.idle":"2023-04-01T07:11:30.761495Z","shell.execute_reply":"2023-04-01T07:11:30.760384Z","shell.execute_reply.started":"2023-04-01T07:11:30.756191Z"},"trusted":true},"outputs":[],"source":["from pytorch_lightning.callbacks import TQDMProgressBar\n","from pytorch_lightning.loggers import CSVLogger"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:12:18.755934Z","iopub.status.busy":"2023-04-01T07:12:18.755515Z","iopub.status.idle":"2023-04-01T07:12:18.772349Z","shell.execute_reply":"2023-04-01T07:12:18.770892Z","shell.execute_reply.started":"2023-04-01T07:12:18.755886Z"},"trusted":true},"outputs":[],"source":["trainer = pl.Trainer(gpus=1,\n","                     precision=32,\n","                     max_epochs = 5,\n","                     auto_lr_find = True,\n","                     #callbacks=[checkpoint,early_stopping_callback],\n","                     #logger = wandb_logger\n","                    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T07:13:56.167755Z","iopub.status.busy":"2023-04-01T07:13:56.167332Z","iopub.status.idle":"2023-04-01T09:05:49.688670Z","shell.execute_reply":"2023-04-01T09:05:49.687728Z","shell.execute_reply.started":"2023-04-01T07:13:56.167719Z"},"trusted":true},"outputs":[],"source":["trainer.fit(model,summary_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:04.861800Z","iopub.status.busy":"2023-04-01T09:06:04.861435Z","iopub.status.idle":"2023-04-01T09:06:09.107873Z","shell.execute_reply":"2023-04-01T09:06:09.106753Z","shell.execute_reply.started":"2023-04-01T09:06:04.861768Z"},"trusted":true},"outputs":[],"source":["#path = model.save('/content/drive/MyDrive/NLP Project/Results/bart_model_5_epoch_unfreeze')\n","path = model.save('bart_model_with_loss_penalty')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:09.412575Z","iopub.status.busy":"2023-04-01T09:06:09.411319Z","iopub.status.idle":"2023-04-01T09:06:09.419066Z","shell.execute_reply":"2023-04-01T09:06:09.417900Z","shell.execute_reply.started":"2023-04-01T09:06:09.412504Z"},"trusted":true},"outputs":[],"source":["def generate_summary(seed_line, model_,num_beam = 4,penalty_length =0.2):\n","\n","  # Put the model on eval mode\n","\n","    model_.to(torch.device('cuda'))\n","    model_.eval()\n","\n","    line = model_.generate_text(seed_line, eval_beams=num_beam,length_penalty = penalty_length)\n","\n","    return line"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:12.668029Z","iopub.status.busy":"2023-04-01T09:06:12.667591Z","iopub.status.idle":"2023-04-01T09:06:14.073015Z","shell.execute_reply":"2023-04-01T09:06:14.071481Z","shell.execute_reply.started":"2023-04-01T09:06:12.667984Z"},"trusted":true},"outputs":[],"source":["model = torch.load('bart_model_with_loss_penalty.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:14.075566Z","iopub.status.busy":"2023-04-01T09:06:14.075132Z","iopub.status.idle":"2023-04-01T09:06:14.081954Z","shell.execute_reply":"2023-04-01T09:06:14.080852Z","shell.execute_reply.started":"2023-04-01T09:06:14.075508Z"},"trusted":true},"outputs":[],"source":["b_model = Model(lr = 2e-5, tokenizer = bart_tokenizer, model = model, params = hparams)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:10:02.416921Z","iopub.status.busy":"2023-04-01T09:10:02.416504Z","iopub.status.idle":"2023-04-01T09:10:02.422340Z","shell.execute_reply":"2023-04-01T09:10:02.421010Z","shell.execute_reply.started":"2023-04-01T09:10:02.416888Z"},"trusted":true},"outputs":[],"source":["num_beams = [3]\n","p_l = [0.2]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:17.894596Z","iopub.status.busy":"2023-04-01T09:06:17.894212Z","iopub.status.idle":"2023-04-01T09:06:30.429784Z","shell.execute_reply":"2023-04-01T09:06:30.428416Z","shell.execute_reply.started":"2023-04-01T09:06:17.894561Z"},"trusted":true},"outputs":[],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:30.434100Z","iopub.status.busy":"2023-04-01T09:06:30.433068Z","iopub.status.idle":"2023-04-01T09:06:51.468975Z","shell.execute_reply":"2023-04-01T09:06:51.467849Z","shell.execute_reply.started":"2023-04-01T09:06:30.434054Z"},"trusted":true},"outputs":[],"source":["import nltk\n","nltk.download('all')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:51.471936Z","iopub.status.busy":"2023-04-01T09:06:51.471118Z","iopub.status.idle":"2023-04-01T09:06:51.485113Z","shell.execute_reply":"2023-04-01T09:06:51.483680Z","shell.execute_reply.started":"2023-04-01T09:06:51.471891Z"},"trusted":true},"outputs":[],"source":["val_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:51.489222Z","iopub.status.busy":"2023-04-01T09:06:51.488608Z","iopub.status.idle":"2023-04-01T09:06:51.497764Z","shell.execute_reply":"2023-04-01T09:06:51.496150Z","shell.execute_reply.started":"2023-04-01T09:06:51.489180Z"},"trusted":true},"outputs":[],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:51.500934Z","iopub.status.busy":"2023-04-01T09:06:51.499979Z","iopub.status.idle":"2023-04-01T09:06:51.513005Z","shell.execute_reply":"2023-04-01T09:06:51.511620Z","shell.execute_reply.started":"2023-04-01T09:06:51.500887Z"},"trusted":true},"outputs":[],"source":["nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:51.515853Z","iopub.status.busy":"2023-04-01T09:06:51.514895Z","iopub.status.idle":"2023-04-01T09:06:51.581684Z","shell.execute_reply":"2023-04-01T09:06:51.580406Z","shell.execute_reply.started":"2023-04-01T09:06:51.515802Z"},"trusted":true},"outputs":[],"source":["from nltk import word_tokenize\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:06:51.583726Z","iopub.status.busy":"2023-04-01T09:06:51.583276Z","iopub.status.idle":"2023-04-01T09:06:51.589481Z","shell.execute_reply":"2023-04-01T09:06:51.587749Z","shell.execute_reply.started":"2023-04-01T09:06:51.583689Z"},"trusted":true},"outputs":[],"source":["from rouge import Rouge\n","scorer = Rouge()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:10:11.790574Z","iopub.status.busy":"2023-04-01T09:10:11.789784Z","iopub.status.idle":"2023-04-01T09:13:48.710365Z","shell.execute_reply":"2023-04-01T09:13:48.709332Z","shell.execute_reply.started":"2023-04-01T09:10:11.790521Z"},"trusted":true},"outputs":[],"source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","# from nltk.translate.meteor_score import meteor_score\n","bleu_1 = 0\n","bleu_2 = 0\n","bleu_3 = 0\n","bleu_4 = 0\n","count = 0\n","weights_1 = (1./1.,)\n","weights_2 = (1./2. , 1./2.)\n","weights_3 = (1./3., 1./3., 1./3.)\n","weights_4 = (1./4., 1./4., 1./4., 1./4.)\n","# met = 0\n","\n","for beam in num_beams:\n","    for length in p_l:\n","        print(\"Actual summ\")\n","        bart_summ = generate_summary(val_inputs,b_model,num_beam=beam,penalty_length=length)\n","        print(\"Val summ\\n\")\n","        print(val_summ)\n","        print(\"Actual Summ\\n\")\n","        print(bart_summ)\n","        bart_scorer = scorer.get_scores(val_summ, bart_summ, avg=True)\n","        print(bart_scorer)\n","\n","        # Evaluate generated summary against reference summary using BLEU and METEOR\n","        reference = str(val_summ)\n","        hypothesis = str(bart_summ)\n","        reference = reference.split()\n","        hypothesis = hypothesis.split()\n","        bleu_1 += sentence_bleu([reference], hypothesis, weights_1) \n","        bleu_2 += sentence_bleu([reference], hypothesis, weights_2)\n","        bleu_3 += sentence_bleu([reference], hypothesis, weights_3)\n","        bleu_4 += sentence_bleu([reference], hypothesis, weights_4)\n","#         ref1 = val_summ\n","#         ref = word_tokenize(ref1)\n","#         hyp1 = bart_summ\n","#         hyp = word_tokenize(hyp1)\n","#         met += nltk.translate.meteor_score.meteor_score([ref], hyp)\n","        count += 1\n","\n","bleu_1 /= count\n","bleu_2 /= count\n","bleu_3 /= count\n","bleu_4 /= count\n","# met /= count\n","\n","print(\"BLEU-1:\", bleu_1)\n","print(\"BLEU-2:\", bleu_2)\n","print(\"BLEU-3:\", bleu_3)\n","print(\"BLEU-4:\", bleu_4)\n","# print(\"METEOR:\", met)"]},{"cell_type":"markdown","metadata":{},"source":["### Run on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:13:48.713496Z","iopub.status.busy":"2023-04-01T09:13:48.712720Z","iopub.status.idle":"2023-04-01T09:26:56.648668Z","shell.execute_reply":"2023-04-01T09:26:56.647672Z","shell.execute_reply.started":"2023-04-01T09:13:48.713453Z"},"trusted":true},"outputs":[],"source":["bart_summ_test = generate_summary(test_inputs,b_model,num_beam=beam,penalty_length=length)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.651115Z","iopub.status.busy":"2023-04-01T09:26:56.650423Z","iopub.status.idle":"2023-04-01T09:26:56.658120Z","shell.execute_reply":"2023-04-01T09:26:56.656999Z","shell.execute_reply.started":"2023-04-01T09:26:56.651075Z"},"trusted":true},"outputs":[],"source":["len(bart_summ_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.661560Z","iopub.status.busy":"2023-04-01T09:26:56.660887Z","iopub.status.idle":"2023-04-01T09:26:56.690640Z","shell.execute_reply":"2023-04-01T09:26:56.689888Z","shell.execute_reply.started":"2023-04-01T09:26:56.661524Z"},"trusted":true},"outputs":[],"source":["bart_summ_test"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the BART summ for test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.692912Z","iopub.status.busy":"2023-04-01T09:26:56.692268Z","iopub.status.idle":"2023-04-01T09:26:56.698510Z","shell.execute_reply":"2023-04-01T09:26:56.697495Z","shell.execute_reply.started":"2023-04-01T09:26:56.692875Z"},"trusted":true},"outputs":[],"source":["list_of_idx = []\n","list_of_summ = []\n","\n","for idx,summ in enumerate(bart_summ_test):\n","  list_of_idx.append(idx)\n","  list_of_summ.append(summ)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.700641Z","iopub.status.busy":"2023-04-01T09:26:56.699827Z","iopub.status.idle":"2023-04-01T09:26:56.711939Z","shell.execute_reply":"2023-04-01T09:26:56.710924Z","shell.execute_reply.started":"2023-04-01T09:26:56.700606Z"},"trusted":true},"outputs":[],"source":["len(list_of_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.716080Z","iopub.status.busy":"2023-04-01T09:26:56.715776Z","iopub.status.idle":"2023-04-01T09:26:56.723452Z","shell.execute_reply":"2023-04-01T09:26:56.722321Z","shell.execute_reply.started":"2023-04-01T09:26:56.716056Z"},"trusted":true},"outputs":[],"source":["res = pd.DataFrame({'Generated_Summary':list_of_summ,'id':list_of_idx})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.726034Z","iopub.status.busy":"2023-04-01T09:26:56.725001Z","iopub.status.idle":"2023-04-01T09:26:56.752106Z","shell.execute_reply":"2023-04-01T09:26:56.750870Z","shell.execute_reply.started":"2023-04-01T09:26:56.725997Z"},"trusted":true},"outputs":[],"source":["res"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.754516Z","iopub.status.busy":"2023-04-01T09:26:56.753624Z","iopub.status.idle":"2023-04-01T09:26:56.775593Z","shell.execute_reply":"2023-04-01T09:26:56.774305Z","shell.execute_reply.started":"2023-04-01T09:26:56.754475Z"},"trusted":true},"outputs":[],"source":["datatext = pd.DataFrame().assign(Original_Text=test_data['Original_Text'], Gold_Summary=test_data['Summary'])\n","datatext.reset_index(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.781404Z","iopub.status.busy":"2023-04-01T09:26:56.780482Z","iopub.status.idle":"2023-04-01T09:26:56.801922Z","shell.execute_reply":"2023-04-01T09:26:56.801004Z","shell.execute_reply.started":"2023-04-01T09:26:56.781370Z"},"trusted":true},"outputs":[],"source":["datatext.drop(['index'],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.803933Z","iopub.status.busy":"2023-04-01T09:26:56.803366Z","iopub.status.idle":"2023-04-01T09:26:56.823202Z","shell.execute_reply":"2023-04-01T09:26:56.822370Z","shell.execute_reply.started":"2023-04-01T09:26:56.803897Z"},"trusted":true},"outputs":[],"source":["result = pd.concat([datatext, res], axis=1, join='inner')\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.824965Z","iopub.status.busy":"2023-04-01T09:26:56.824469Z","iopub.status.idle":"2023-04-01T09:26:56.832156Z","shell.execute_reply":"2023-04-01T09:26:56.831009Z","shell.execute_reply.started":"2023-04-01T09:26:56.824909Z"},"trusted":true},"outputs":[],"source":["result['Generated_Summary'][30]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.834065Z","iopub.status.busy":"2023-04-01T09:26:56.833406Z","iopub.status.idle":"2023-04-01T09:26:56.843915Z","shell.execute_reply":"2023-04-01T09:26:56.842740Z","shell.execute_reply.started":"2023-04-01T09:26:56.834024Z"},"trusted":true},"outputs":[],"source":["result['Gold_Summary'][30]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.846413Z","iopub.status.busy":"2023-04-01T09:26:56.845463Z","iopub.status.idle":"2023-04-01T09:26:56.926703Z","shell.execute_reply":"2023-04-01T09:26:56.925735Z","shell.execute_reply.started":"2023-04-01T09:26:56.846378Z"},"trusted":true},"outputs":[],"source":["result.to_csv('predictions.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.928391Z","iopub.status.busy":"2023-04-01T09:26:56.928057Z","iopub.status.idle":"2023-04-01T09:26:56.933274Z","shell.execute_reply":"2023-04-01T09:26:56.931876Z","shell.execute_reply.started":"2023-04-01T09:26:56.928363Z"},"trusted":true},"outputs":[],"source":["from rouge import Rouge\n","rouge = Rouge()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:26:56.935796Z","iopub.status.busy":"2023-04-01T09:26:56.935132Z","iopub.status.idle":"2023-04-01T09:27:05.213846Z","shell.execute_reply":"2023-04-01T09:27:05.212653Z","shell.execute_reply.started":"2023-04-01T09:26:56.935759Z"},"trusted":true},"outputs":[],"source":["scores = rouge.get_scores(list(result['Generated_Summary']), list(result['Gold_Summary']), avg=True)\n","\n","print(scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:27:05.216179Z","iopub.status.busy":"2023-04-01T09:27:05.215707Z","iopub.status.idle":"2023-04-01T09:27:05.222585Z","shell.execute_reply":"2023-04-01T09:27:05.221524Z","shell.execute_reply.started":"2023-04-01T09:27:05.216116Z"},"trusted":true},"outputs":[],"source":["y_pred = list(result['Generated_Summary'])\n","y_true = list(result['Gold_Summary'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:27:05.224733Z","iopub.status.busy":"2023-04-01T09:27:05.224080Z","iopub.status.idle":"2023-04-01T09:27:05.234634Z","shell.execute_reply":"2023-04-01T09:27:05.233317Z","shell.execute_reply.started":"2023-04-01T09:27:05.224690Z"},"trusted":true},"outputs":[],"source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","from scipy import spatial\n","nltk.download('wordnet')\n","from nltk.translate.meteor_score import meteor_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:27:05.236888Z","iopub.status.busy":"2023-04-01T09:27:05.236292Z","iopub.status.idle":"2023-04-01T09:27:05.255277Z","shell.execute_reply":"2023-04-01T09:27:05.254058Z","shell.execute_reply.started":"2023-04-01T09:27:05.236835Z"},"trusted":true},"outputs":[],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:27:05.257536Z","iopub.status.busy":"2023-04-01T09:27:05.257126Z","iopub.status.idle":"2023-04-01T09:27:05.267736Z","shell.execute_reply":"2023-04-01T09:27:05.266782Z","shell.execute_reply.started":"2023-04-01T09:27:05.257502Z"},"trusted":true},"outputs":[],"source":["from nltk import word_tokenize\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-01T09:27:05.270040Z","iopub.status.busy":"2023-04-01T09:27:05.269020Z","iopub.status.idle":"2023-04-01T09:27:06.845068Z","shell.execute_reply":"2023-04-01T09:27:06.844003Z","shell.execute_reply.started":"2023-04-01T09:27:05.270005Z"},"trusted":true},"outputs":[],"source":["bleu_1 = 0\n","bleu_2 = 0\n","bleu_3 = 0\n","bleu_4 = 0\n","count = 0\n","weights_1 = (1./1.,)\n","weights_2 = (1./2. , 1./2.)\n","weights_3 = (1./3., 1./3., 1./3.)\n","weights_4 = (1./4., 1./4., 1./4., 1./4.)\n","# met = 0\n","\n","for reference, hypothesis in zip(y_true, y_pred):\n","    ref = word_tokenize(reference)\n","    hyp = word_tokenize(hypothesis)\n","#     met += nltk.translate.meteor_score.meteor_score([ref], hyp)\n","    \n","    reference = reference.split()\n","    hypothesis = hypothesis.split()\n","    bleu_1 += sentence_bleu([reference], hypothesis, weights_1) \n","    bleu_2 += sentence_bleu([reference], hypothesis, weights_2)\n","    bleu_3 += sentence_bleu([reference], hypothesis, weights_3)\n","    bleu_4 += sentence_bleu([reference], hypothesis, weights_4)\n","    count += 1\n","\n","bleu_1 = bleu_1/count\n","bleu_2 = bleu_2/count\n","bleu_3 = bleu_3/count\n","bleu_4 = bleu_4/count\n","\n","# met = met/count\n","\n","print(\"BLEU-1:\", bleu_1)\n","print(\"BLEU-2:\", bleu_2)\n","print(\"BLEU-3:\", bleu_3)\n","print(\"BLEU-4:\", bleu_4)\n","# print(\"METEOR:\", met)"]},{"cell_type":"markdown","metadata":{},"source":["### "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
